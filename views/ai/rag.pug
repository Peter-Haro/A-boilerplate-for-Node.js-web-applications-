extends ../layout

block content
  .pb-2.mt-2.mb-4.border-bottom
    h2
      i.fas.fa-robot.iconpadding
      | Retrieval-Augmented Generation (RAG)

  .btn-group.d-flex(role='group')
    a.btn.btn-primary.w-100(href='https://api.together.ai/', target='_blank')
      i.fas.fa-hexagon-nodes.fa-sm.iconpadding
      | Together.ai Inference API
    a.btn.btn-primary.w-100(href='https://huggingface.co/docs/api-inference/index', target='_blank')
      i.fas.fa-arrow-up-1-9.fa-sm.iconpadding
      | Hugging Face Inference API
    a.btn.btn-primary.w-100(href='https://www.mongodb.com/docs/atlas/atlas-vector-search/', target='_blank')
      i.fas.fa-database.fa-sm.iconpadding
      | MongoDB Vector Search
    a.btn.btn-primary.w-100(href='https://js.langchain.com/docs/integrations/vectorstores/mongodb_atlas', target='_blank')
      i.fas.fa-link.fa-sm.iconpadding
      | LangChain.js

  .container
    .row.g-4.mt-1
      .col-md-6
        .card.shadow-sm
          .card-body.bg-light
            h3.text-center Ingested Files
            if ingestedFiles && ingestedFiles.length > 0
              table.table.table-striped.table-bordered
                thead
                  tr
                    th.text-center File Name
                tbody
                  each file in ingestedFiles
                    tr
                      td.text-center= file
            else
              p.text-muted.text-center No files ingested yet.
            p.text-center
              | You can add files to the 'rag_input' folder on the server and process them for RAG.

            form(action='/ai/rag/ingest', method='POST')
              input(type='hidden', name='_csrf', value=_csrf)
              button.btn.btn-primary.btn-lg.w-100.mt-3(type='submit')
                i.fas.fa-sync.fa-sm.iconpadding
                | Ingest Files

      .col-md-6
        .card.shadow-sm
          .card-body.bg-light
            h3.text-center Ask a Question
            p.text-left Try asking a question about information in the ingested RAG documents that were not part of the LLM's training data. For example,
            ul.list-group.list-group-flush
              li.list-group-item How much did Amazon make in 2024?
              li.list-group-item How much debt did Amazon have at the end of 2024?
              li.list-group-item How much was Microsoft's advertising expense in fiscal year 2024?
              li.list-group-item What is the total amount of stock that Microsoft gave to its employees in fiscal year 2024?
            br
            form(action='/ai/rag/ask', method='POST')
              input(type='hidden', name='_csrf', value=_csrf)
              .form-group
                label(for='question') Your Question:
                textarea#question.form-control(name='question', rows='3', maxlength=maxInputLength, autofocus, required)= question
              button.btn.btn-primary.btn-lg.w-100.mt-3(type='submit')
                i.fas.fa-paper-plane.fa-sm.iconpadding
                | Ask

    if ragResponse || llmResponse
      .row.g-4.mt-1
        if ragResponse
          .col-md-6
            .card.shadow-sm
              .card-body.bg-light
                h3.text-center
                  i.fas.fa-database.fa-sm.iconpadding
                  | RAG LLM Response
                .response-box
                  pre.text-wrap= ragResponse
        if llmResponse
          .col-md-6
            .card.shadow-sm
              .card-body.bg-light
                h3.text-center
                  i.fas.fa-brain.fa-sm.iconpadding
                  | No-RAG LLM Response
                .response-box
                  pre.text-wrap= llmResponse

    hr.border-primary.my-4

    .card.shadow-sm
      .card-body.bg-light
        .row.align-items-top.g-4
          .col-md-5
            h4
              | Boilerplate RAG with Semantic Caching
            p
              | You can start with this base RAG implementation and extend it for your project so you can focus on your business logic and not on boilerplate code. For this implementation, we are using:
              ul
                li LangChain.js ü¶úÔ∏èüîó for the pipeline and integrations
                li Sentence-transformers/all-MiniLM-L6-v2 embedding model through the Hugging Face ü§ó Inference API
                li Llama-3.3-70B-Instruct-Turbo ü¶ô hosted by Together.ai as the LLM
                li PDF.js to extract text from PDF files
                li MongoDB Atlas Vector Search as the vector DB for RAG document storage and LLM caching
                li MongoDB as the key-value store for embedding caching
            br
            br
            p
              | The commented source code including helper functions are in controllers/ai.js
          .col-md-7.text-center
            img.img-fluid.w-100.rounded.mx-auto.d-block(src='https://i.imgur.com/zqIxnEj.png', alt='RAG Demo Block Diagram')
    hr.border-primary.my-4
    style.
      .response-box {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 5px;
        margin-top: 10px;
      }
      pre.text-wrap {
        white-space: pre-wrap;
        word-wrap: break-word;
      }
